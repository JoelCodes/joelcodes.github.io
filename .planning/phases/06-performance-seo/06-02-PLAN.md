---
phase: 06-performance-seo
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - astro.config.mjs
  - package.json
autonomous: true

must_haves:
  truths:
    - "Sitemap.xml is generated at /sitemap-index.xml during build"
    - "Robots.txt exists at /robots.txt with sitemap reference"
    - "Robots.txt allows all crawlers access to public pages"
  artifacts:
    - path: "astro.config.mjs"
      provides: "Sitemap and robots.txt integration configuration"
      contains: "@astrojs/sitemap"
    - path: "dist/sitemap-index.xml"
      provides: "Generated sitemap index"
    - path: "dist/robots.txt"
      provides: "Generated robots.txt"
  key_links:
    - from: "astro.config.mjs"
      to: "@astrojs/sitemap"
      via: "integrations array"
      pattern: "sitemap\\(\\)"
---

<objective>
Configure automatic sitemap.xml and robots.txt generation for search engine discovery

Purpose: Enable search engines to efficiently crawl and index all pages
Output: Sitemap and robots.txt generated during build via Astro integrations
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/06-performance-seo/06-CONTEXT.md
@.planning/phases/06-performance-seo/06-RESEARCH.md

@astro.config.mjs
@package.json
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install sitemap and robots.txt packages</name>
  <files>package.json</files>
  <action>
Install the official Astro sitemap integration and robots.txt package:

```bash
npm install @astrojs/sitemap astro-robots-txt
```

These are the standard packages per RESEARCH.md:
- @astrojs/sitemap: Official Astro integration for sitemap generation
- astro-robots-txt: Popular package that auto-syncs sitemap location
  </action>
  <verify>
Run: npm ls @astrojs/sitemap astro-robots-txt
Both packages appear in dependency tree
  </verify>
  <done>Sitemap and robots.txt packages installed</done>
</task>

<task type="auto">
  <name>Task 2: Configure sitemap and robots.txt in Astro config</name>
  <files>astro.config.mjs</files>
  <action>
Update astro.config.mjs to add sitemap and robots.txt integrations:

1. Import both packages at top:
   ```javascript
   import sitemap from '@astrojs/sitemap';
   import robotsTxt from 'astro-robots-txt';
   ```

2. Update site URL to canonical domain:
   ```javascript
   site: 'https://joelshinness.com',
   ```

3. Add to integrations array (after expressiveCode and mdx):
   ```javascript
   integrations: [
     expressiveCode(),
     mdx(),
     sitemap({
       changefreq: 'weekly',
       priority: 0.7,
       lastmod: new Date(),
     }),
     robotsTxt({
       sitemap: true,  // Auto-adds Sitemap: URL to robots.txt
       policy: [
         {
           userAgent: '*',
           allow: '/',
         },
       ],
     }),
   ],
   ```

Note: Keep the commented base path line for non-custom-domain deployments.
  </action>
  <verify>
Run: npm run build
Check: ls dist/sitemap*.xml (should show sitemap files)
Check: cat dist/robots.txt (should show User-agent and Sitemap lines)
  </verify>
  <done>Sitemap and robots.txt generate correctly during build</done>
</task>

</tasks>

<verification>
After all tasks:
1. npm run build succeeds
2. dist/sitemap-index.xml exists and contains sitemap URLs
3. dist/robots.txt exists and contains:
   - User-agent: *
   - Allow: /
   - Sitemap: https://joelshinness.com/sitemap-index.xml
4. All public pages appear in sitemap (index, portfolio, contact, blog)
</verification>

<success_criteria>
- @astrojs/sitemap installed and configured
- astro-robots-txt installed and configured
- Build produces sitemap-index.xml
- Build produces robots.txt with sitemap reference
- Site URL set to canonical domain (joelshinness.com)
</success_criteria>

<output>
After completion, create `.planning/phases/06-performance-seo/06-02-SUMMARY.md`
</output>
